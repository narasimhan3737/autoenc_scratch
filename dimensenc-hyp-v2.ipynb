{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "import pickle\n",
    "device = T.device(\"cuda:0\")  # apply to Tensor or Module\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "#path = \"/mnt/d/Work/Dissertation/Preprocessing/VNIR-Field1/\"\n",
    "#onlyfiles = [ f for f in listdir(path) if isfile(join(path,f)) ]\n",
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0901, 0.0905, 0.0909,  ..., 0.0708, 0.0704, 0.0711],\n",
       "          [0.1018, 0.0931, 0.0869,  ..., 0.0703, 0.0711, 0.0657],\n",
       "          [0.0925, 0.0898, 0.0857,  ..., 0.0711, 0.0665, 0.0582],\n",
       "          ...,\n",
       "          [0.0639, 0.0603, 0.0551,  ..., 0.0586, 0.0780, 0.1233],\n",
       "          [0.0630, 0.0481, 0.0684,  ..., 0.0786, 0.1079, 0.1404],\n",
       "          [0.0623, 0.0571, 0.0522,  ..., 0.0702, 0.1402, 0.2073]],\n",
       "\n",
       "         [[0.0966, 0.0971, 0.1000,  ..., 0.0726, 0.0717, 0.0763],\n",
       "          [0.1085, 0.1008, 0.1035,  ..., 0.0714, 0.0763, 0.0812],\n",
       "          [0.0949, 0.1021, 0.1045,  ..., 0.0763, 0.0804, 0.0879],\n",
       "          ...,\n",
       "          [0.0612, 0.0580, 0.0555,  ..., 0.0677, 0.0934, 0.1563],\n",
       "          [0.0615, 0.0543, 0.0558,  ..., 0.0960, 0.1360, 0.1765],\n",
       "          [0.0634, 0.0579, 0.0566,  ..., 0.0713, 0.1809, 0.2196]],\n",
       "\n",
       "         [[0.0996, 0.0891, 0.1016,  ..., 0.0691, 0.0695, 0.0647],\n",
       "          [0.1112, 0.0994, 0.1025,  ..., 0.0698, 0.0647, 0.0610],\n",
       "          [0.0944, 0.0976, 0.1035,  ..., 0.0647, 0.0616, 0.0560],\n",
       "          ...,\n",
       "          [0.0580, 0.0598, 0.0611,  ..., 0.0739, 0.0925, 0.1393],\n",
       "          [0.0595, 0.0609, 0.0625,  ..., 0.0962, 0.1335, 0.1725],\n",
       "          [0.0642, 0.0622, 0.0482,  ..., 0.0887, 0.1860, 0.2300]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3284, 0.3725, 0.3537,  ..., 0.2822, 0.2814, 0.2670],\n",
       "          [0.3345, 0.3591, 0.3487,  ..., 0.2825, 0.2670, 0.2537],\n",
       "          [0.3829, 0.3491, 0.3412,  ..., 0.2670, 0.2558, 0.2355],\n",
       "          ...,\n",
       "          [0.2924, 0.3127, 0.3331,  ..., 0.2879, 0.2752, 0.2598],\n",
       "          [0.3473, 0.3095, 0.3632,  ..., 0.2841, 0.3055, 0.3367],\n",
       "          [0.3457, 0.3001, 0.3070,  ..., 0.2525, 0.3757, 0.4012]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0843, 0.0833, 0.0761,  ..., 0.0934, 0.0923, 0.0946],\n",
       "          [0.0802, 0.0762, 0.0913,  ..., 0.0904, 0.0855, 0.0931],\n",
       "          [0.0763, 0.0733, 0.0762,  ..., 0.0935, 0.1016, 0.0770],\n",
       "          ...,\n",
       "          [0.0696, 0.0708, 0.0662,  ..., 0.1130, 0.2123, 0.1491],\n",
       "          [0.0796, 0.0623, 0.0612,  ..., 0.1547, 0.1610, 0.1835],\n",
       "          [0.0751, 0.0674, 0.0784,  ..., 0.1105, 0.2011, 0.2026]],\n",
       "\n",
       "         [[0.0802, 0.0855, 0.0794,  ..., 0.1051, 0.1003, 0.1041],\n",
       "          [0.0798, 0.0689, 0.0793,  ..., 0.0814, 0.1115, 0.1124],\n",
       "          [0.0661, 0.0941, 0.0895,  ..., 0.0943, 0.1076, 0.1093],\n",
       "          ...,\n",
       "          [0.0711, 0.0703, 0.0660,  ..., 0.1201, 0.2191, 0.1534],\n",
       "          [0.0730, 0.0763, 0.0807,  ..., 0.1672, 0.1674, 0.1926],\n",
       "          [0.0875, 0.0680, 0.0743,  ..., 0.1105, 0.2009, 0.2289]],\n",
       "\n",
       "         [[0.0744, 0.0791, 0.0712,  ..., 0.0941, 0.1015, 0.1088],\n",
       "          [0.0888, 0.0734, 0.0753,  ..., 0.0916, 0.1024, 0.0922],\n",
       "          [0.0838, 0.0964, 0.0874,  ..., 0.0897, 0.1098, 0.1063],\n",
       "          ...,\n",
       "          [0.0659, 0.0627, 0.0615,  ..., 0.1167, 0.2375, 0.1820],\n",
       "          [0.0641, 0.0562, 0.0645,  ..., 0.1585, 0.1758, 0.2088],\n",
       "          [0.0777, 0.0640, 0.0768,  ..., 0.1135, 0.2285, 0.2365]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.2723, 0.3128, 0.3094,  ..., 0.3549, 0.3746, 0.3397],\n",
       "          [0.3054, 0.3112, 0.2800,  ..., 0.3840, 0.3113, 0.3462],\n",
       "          [0.3246, 0.2908, 0.3501,  ..., 0.3431, 0.3523, 0.3346],\n",
       "          ...,\n",
       "          [0.2873, 0.3133, 0.3262,  ..., 0.3229, 0.4167, 0.4609],\n",
       "          [0.3112, 0.2985, 0.3664,  ..., 0.3756, 0.3792, 0.4095],\n",
       "          [0.3283, 0.3055, 0.3422,  ..., 0.3300, 0.3832, 0.4359]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0593, 0.0552, 0.0555,  ..., 0.0497, 0.0475, 0.0523],\n",
       "          [0.0463, 0.0588, 0.0507,  ..., 0.0571, 0.0605, 0.0495],\n",
       "          [0.0475, 0.0613, 0.0566,  ..., 0.0501, 0.0605, 0.0484],\n",
       "          ...,\n",
       "          [0.0559, 0.0467, 0.0529,  ..., 0.0533, 0.1922, 0.1256],\n",
       "          [0.0547, 0.0553, 0.0579,  ..., 0.0737, 0.1117, 0.1604],\n",
       "          [0.0510, 0.0514, 0.0577,  ..., 0.0797, 0.1128, 0.1721]],\n",
       "\n",
       "         [[0.0558, 0.0495, 0.0564,  ..., 0.0527, 0.0525, 0.0492],\n",
       "          [0.0494, 0.0711, 0.0532,  ..., 0.0464, 0.0492, 0.0617],\n",
       "          [0.0683, 0.0646, 0.0617,  ..., 0.0496, 0.0538, 0.0682],\n",
       "          ...,\n",
       "          [0.0663, 0.0523, 0.0532,  ..., 0.0566, 0.0924, 0.1454],\n",
       "          [0.0576, 0.0499, 0.0590,  ..., 0.0592, 0.0853, 0.1211],\n",
       "          [0.0489, 0.0446, 0.0586,  ..., 0.0661, 0.0820, 0.1047]],\n",
       "\n",
       "         [[0.0592, 0.0614, 0.0543,  ..., 0.0457, 0.0581, 0.0579],\n",
       "          [0.0573, 0.0634, 0.0591,  ..., 0.0630, 0.0473, 0.0535],\n",
       "          [0.0576, 0.0556, 0.0477,  ..., 0.0432, 0.0531, 0.0592],\n",
       "          ...,\n",
       "          [0.0623, 0.0505, 0.0550,  ..., 0.0607, 0.0921, 0.1455],\n",
       "          [0.0482, 0.0560, 0.0580,  ..., 0.0601, 0.0873, 0.1253],\n",
       "          [0.0512, 0.0612, 0.0501,  ..., 0.0587, 0.0752, 0.1063]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.2725, 0.2610, 0.2475,  ..., 0.3140, 0.2644, 0.2822],\n",
       "          [0.2546, 0.2683, 0.2868,  ..., 0.3078, 0.2894, 0.3110],\n",
       "          [0.2458, 0.2755, 0.2200,  ..., 0.3078, 0.2858, 0.2863],\n",
       "          ...,\n",
       "          [0.3141, 0.2725, 0.3095,  ..., 0.3613, 0.3038, 0.3666],\n",
       "          [0.3391, 0.3401, 0.2960,  ..., 0.3260, 0.3440, 0.3379],\n",
       "          [0.3391, 0.2976, 0.3231,  ..., 0.3386, 0.3127, 0.3277]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0468, 0.0357, 0.0441,  ..., 0.0255, 0.0372, 0.0554],\n",
       "          [0.0558, 0.0591, 0.0407,  ..., 0.0419, 0.0363, 0.0411],\n",
       "          [0.0581, 0.0368, 0.0509,  ..., 0.0380, 0.0395, 0.0372],\n",
       "          ...,\n",
       "          [0.0550, 0.0491, 0.0375,  ..., 0.0447, 0.0528, 0.0894],\n",
       "          [0.0467, 0.0477, 0.0464,  ..., 0.0313, 0.0428, 0.1148],\n",
       "          [0.0419, 0.0490, 0.0503,  ..., 0.0371, 0.0491, 0.1212]],\n",
       "\n",
       "         [[0.0452, 0.0545, 0.0379,  ..., 0.0524, 0.0339, 0.0427],\n",
       "          [0.0468, 0.0535, 0.0501,  ..., 0.0536, 0.0469, 0.0437],\n",
       "          [0.0498, 0.0607, 0.0460,  ..., 0.0481, 0.0490, 0.0463],\n",
       "          ...,\n",
       "          [0.0475, 0.0600, 0.0484,  ..., 0.0500, 0.0586, 0.1016],\n",
       "          [0.0523, 0.0499, 0.0522,  ..., 0.0357, 0.0699, 0.1224],\n",
       "          [0.0433, 0.0518, 0.0567,  ..., 0.0492, 0.0677, 0.1531]],\n",
       "\n",
       "         [[0.0430, 0.0382, 0.0453,  ..., 0.0521, 0.0608, 0.0411],\n",
       "          [0.0522, 0.0420, 0.0510,  ..., 0.0460, 0.0465, 0.0483],\n",
       "          [0.0524, 0.0477, 0.0443,  ..., 0.0375, 0.0480, 0.0398],\n",
       "          ...,\n",
       "          [0.0534, 0.0498, 0.0504,  ..., 0.0491, 0.0632, 0.1054],\n",
       "          [0.0552, 0.0554, 0.0528,  ..., 0.0446, 0.0702, 0.1412],\n",
       "          [0.0637, 0.0534, 0.0548,  ..., 0.0503, 0.0850, 0.1706]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.2886, 0.2703, 0.2157,  ..., 0.2578, 0.2324, 0.2052],\n",
       "          [0.3253, 0.3032, 0.2687,  ..., 0.2210, 0.2455, 0.2264],\n",
       "          [0.2784, 0.2922, 0.3055,  ..., 0.2512, 0.2657, 0.1923],\n",
       "          ...,\n",
       "          [0.2997, 0.2781, 0.3035,  ..., 0.2467, 0.2531, 0.2791],\n",
       "          [0.2891, 0.3028, 0.2932,  ..., 0.2514, 0.2717, 0.2297],\n",
       "          [0.3315, 0.2837, 0.3022,  ..., 0.2394, 0.2357, 0.2020]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0533, 0.0580, 0.0548,  ..., 0.0461, 0.0390, 0.0555],\n",
       "          [0.0482, 0.0544, 0.0544,  ..., 0.0496, 0.0561, 0.0567],\n",
       "          [0.0547, 0.0526, 0.0510,  ..., 0.0491, 0.0449, 0.0470],\n",
       "          ...,\n",
       "          [0.0497, 0.0470, 0.0555,  ..., 0.0889, 0.1397, 0.1563],\n",
       "          [0.0446, 0.0446, 0.0487,  ..., 0.0836, 0.1992, 0.2083],\n",
       "          [0.0463, 0.0451, 0.0509,  ..., 0.0771, 0.1304, 0.1388]],\n",
       "\n",
       "         [[0.0466, 0.0524, 0.0513,  ..., 0.0550, 0.0590, 0.0544],\n",
       "          [0.0532, 0.0500, 0.0510,  ..., 0.0547, 0.0605, 0.0611],\n",
       "          [0.0496, 0.0529, 0.0521,  ..., 0.0490, 0.0644, 0.0527],\n",
       "          ...,\n",
       "          [0.0519, 0.0482, 0.0435,  ..., 0.1075, 0.1539, 0.1657],\n",
       "          [0.0460, 0.0519, 0.0501,  ..., 0.0902, 0.2071, 0.2052],\n",
       "          [0.0528, 0.0539, 0.0478,  ..., 0.0912, 0.1453, 0.1501]],\n",
       "\n",
       "         [[0.0454, 0.0506, 0.0534,  ..., 0.0549, 0.0564, 0.0449],\n",
       "          [0.0528, 0.0494, 0.0526,  ..., 0.0559, 0.0512, 0.0549],\n",
       "          [0.0512, 0.0502, 0.0481,  ..., 0.0542, 0.0489, 0.0618],\n",
       "          ...,\n",
       "          [0.0503, 0.0513, 0.0545,  ..., 0.1013, 0.1567, 0.1744],\n",
       "          [0.0526, 0.0539, 0.0522,  ..., 0.0905, 0.2217, 0.2298],\n",
       "          [0.0501, 0.0494, 0.0475,  ..., 0.0904, 0.1304, 0.1527]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3840, 0.3090, 0.3322,  ..., 0.3253, 0.3620, 0.3121],\n",
       "          [0.3422, 0.3403, 0.3300,  ..., 0.3328, 0.2949, 0.3648],\n",
       "          [0.3274, 0.3244, 0.3382,  ..., 0.2694, 0.3272, 0.3356],\n",
       "          ...,\n",
       "          [0.3716, 0.3725, 0.3938,  ..., 0.3130, 0.3629, 0.3802],\n",
       "          [0.3746, 0.3904, 0.3892,  ..., 0.3264, 0.3913, 0.4608],\n",
       "          [0.3626, 0.3661, 0.3847,  ..., 0.3293, 0.3478, 0.3670]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0521, 0.0519, 0.0508,  ..., 0.0453, 0.0446, 0.0382],\n",
       "          [0.0444, 0.0506, 0.0479,  ..., 0.0597, 0.0338, 0.0594],\n",
       "          [0.0518, 0.0492, 0.0488,  ..., 0.0488, 0.0597, 0.0551],\n",
       "          ...,\n",
       "          [0.0563, 0.0533, 0.0532,  ..., 0.0475, 0.0506, 0.0482],\n",
       "          [0.0540, 0.0521, 0.0548,  ..., 0.0520, 0.0551, 0.0487],\n",
       "          [0.0529, 0.0591, 0.0532,  ..., 0.0478, 0.0492, 0.0470]],\n",
       "\n",
       "         [[0.0565, 0.0592, 0.0590,  ..., 0.0527, 0.0553, 0.0535],\n",
       "          [0.0561, 0.0586, 0.0597,  ..., 0.0473, 0.0450, 0.0570],\n",
       "          [0.0541, 0.0679, 0.0596,  ..., 0.0597, 0.0473, 0.0450],\n",
       "          ...,\n",
       "          [0.0570, 0.0577, 0.0507,  ..., 0.0455, 0.0456, 0.0461],\n",
       "          [0.0509, 0.0608, 0.0558,  ..., 0.0431, 0.0406, 0.0440],\n",
       "          [0.0295, 0.0453, 0.0558,  ..., 0.0464, 0.0467, 0.0468]],\n",
       "\n",
       "         [[0.0554, 0.0549, 0.0527,  ..., 0.0597, 0.0478, 0.0445],\n",
       "          [0.0495, 0.0508, 0.0676,  ..., 0.0562, 0.0463, 0.0393],\n",
       "          [0.0468, 0.0517, 0.0555,  ..., 0.0398, 0.0491, 0.0433],\n",
       "          ...,\n",
       "          [0.0532, 0.0536, 0.0576,  ..., 0.0461, 0.0453, 0.0508],\n",
       "          [0.0504, 0.0527, 0.0529,  ..., 0.0386, 0.0451, 0.0439],\n",
       "          [0.0616, 0.0470, 0.0511,  ..., 0.0470, 0.0467, 0.0486]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3875, 0.3077, 0.2934,  ..., 0.2517, 0.2582, 0.2713],\n",
       "          [0.3385, 0.3283, 0.3259,  ..., 0.2796, 0.2987, 0.3440],\n",
       "          [0.3393, 0.3056, 0.2937,  ..., 0.3157, 0.2305, 0.2514],\n",
       "          ...,\n",
       "          [0.2927, 0.3257, 0.3452,  ..., 0.2592, 0.2661, 0.2758],\n",
       "          [0.2668, 0.3330, 0.2980,  ..., 0.2757, 0.2730, 0.3267],\n",
       "          [0.2890, 0.2895, 0.3016,  ..., 0.2758, 0.2851, 0.2877]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open(\"periya_images.pickle\",\"wb\") as f:\n",
    "    pickle.dump(crop_images,f)\"\"\"\n",
    "\n",
    "with open(\"periya_images.pickle\",\"rb\") as f:\n",
    "    crop_images=pickle.load(f)\n",
    "crop_images = torch.tensor(crop_images,dtype=T.float32)\n",
    "crop_images.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 270, 164, 164])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_cudnn_initialization():\n",
    "    s = 32\n",
    "    dev = torch.device('cuda')\n",
    "    torch.nn.functional.conv2d(torch.zeros(s, s, s, s, device=dev), torch.zeros(s, s, s, s, device=dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "\n",
    "class Net(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.enc1 = T.nn.Conv2d(270, 350, 3)  # 64-16-2-16-64\n",
    "    self.enc2 = T.nn.Conv2d(350, 400, 3) \n",
    "    self.enc3 = T.nn.Conv2d(400, 540 , 3)\n",
    "    self.enc4 = T.nn.Conv2d(540, 610 , 3)\n",
    "    self.enc5 = T.nn.Conv2d(610, 810 , 3)\n",
    "    self.enc6 = T.nn.Conv2d(810, 810 , 3)\n",
    "    self.enc7 = T.nn.Conv2d(810, 810 , 3)\n",
    "    self.enc8 = T.nn.Conv2d(810, 810 , 3)\n",
    "    self.enc9 = T.nn.Conv2d(810, 810 , 3)\n",
    "    self.pool = T.nn.MaxPool2d(2, return_indices=True)\n",
    "    self.unpool=T.nn.MaxUnpool2d(2)\n",
    "    self.dec0 = T.nn.ConvTranspose2d(810, 810, 3)\n",
    "    self.dec1 = T.nn.ConvTranspose2d(810, 810, 3)\n",
    "    self.dec2 = T.nn.ConvTranspose2d(810, 810, 3)\n",
    "    self.dec3 = T.nn.ConvTranspose2d(810, 810, 3)\n",
    "    self.dec4 = T.nn.ConvTranspose2d(810, 610, 3)\n",
    "    self.dec5 = T.nn.ConvTranspose2d(610, 540, 3)\n",
    "    self.dec6 = T.nn.ConvTranspose2d(540, 400, 3)\n",
    "    self.dec7 = T.nn.ConvTranspose2d(400, 350, 3)\n",
    "    self.dec8 = T.nn.ConvTranspose2d(350, 270, 3)\n",
    "  \n",
    "    T.nn.init.xavier_uniform_(self.enc1.weight)\n",
    "    T.nn.init.zeros_(self.enc1.bias)\n",
    "    T.nn.init.xavier_uniform_(self.enc2.weight)\n",
    "    T.nn.init.zeros_(self.enc2.bias)\n",
    "    T.nn.init.xavier_uniform_(self.dec1.weight)\n",
    "    T.nn.init.zeros_(self.dec1.bias)\n",
    "    T.nn.init.xavier_uniform_(self.dec2.weight)\n",
    "    T.nn.init.zeros_(self.dec2.bias)\n",
    "\n",
    "  def encode(self, x):\n",
    "    \n",
    "    z = T.relu(self.enc1(x))\n",
    "    z=self.pool(z)\n",
    "    z = T.relu(self.enc2(z))\n",
    "    z=self.pool(z)\n",
    "    z = T.relu(self.enc3(z))\n",
    "    z=self.pool(z)\n",
    "    z = T.relu(self.enc4(z))\n",
    "    z=self.pool(z)\n",
    "    z = T.relu(self.enc5(z))\n",
    "    z=self.pool(z)\n",
    "    #z = T.relu(self.enc6(z))\n",
    "    #z = T.relu(self.enc7(z))\n",
    "    #z = T.relu(self.enc8(z))\n",
    "    #z = T.relu(self.enc9(z))\n",
    "     # act depends on scenario\n",
    "    return z.to(device)\n",
    "\n",
    "  def decode(self, x):\n",
    "    #z = T.relu(self.dec1(x))\n",
    "    #z = T.relu(self.dec1(z))\n",
    "    #z = T.relu(self.dec2(z))\n",
    "    #z = T.relu(self.dec3(z))\n",
    "    \n",
    "    z = T.relu(self.dec4(x))\n",
    "    z = self.unpool(z)\n",
    "    z = T.relu(self.dec5(z))  # no activation\n",
    "    z = self.unpool(z)\n",
    "    z = T.relu(self.dec6(z))\n",
    "    z=self.unpool(z)\n",
    "    z = T.relu(self.dec7(z))\n",
    "    z=self.unpool(z)\n",
    "    z = self.dec8(z)\n",
    "    \n",
    "    return z   \n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    z = self.encode(x)\n",
    "    z = self.decode(z)\n",
    "    return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Autoenc\\hyperspectral-autoencoders-master\\autoenc_scratch\\dimensenc-hyp-v2.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000004?line=0'>1</a>\u001b[0m net\u001b[39m=\u001b[39mNet()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000004?line=1'>2</a>\u001b[0m inpu\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m270\u001b[39m,\u001b[39m164\u001b[39m,\u001b[39m164\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000004?line=2'>3</a>\u001b[0m out\u001b[39m=\u001b[39mnet(inpu)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000004?line=3'>4</a>\u001b[0m out\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Work\\Autoenc\\hyperspectral-autoencoders-master\\autoenc_scratch\\dimensenc-hyp-v2.ipynb Cell 4'\u001b[0m in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=72'>73</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=74'>75</a>\u001b[0m   z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=75'>76</a>\u001b[0m   z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=76'>77</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[1;32md:\\Work\\Autoenc\\hyperspectral-autoencoders-master\\autoenc_scratch\\dimensenc-hyp-v2.ipynb Cell 4'\u001b[0m in \u001b[0;36mNet.encode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=37'>38</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=38'>39</a>\u001b[0m z\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(z)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=39'>40</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc2(z))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=40'>41</a>\u001b[0m z\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(z)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Autoenc/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003?line=41'>42</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc3(z))\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=455'>456</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=456'>457</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=448'>449</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=449'>450</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=450'>451</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=451'>452</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=452'>453</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda/lib/site-packages/torch/nn/modules/conv.py?line=453'>454</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "net=Net().to(device)\n",
    "inpu=torch.rand(1,270,164,164).to(device)\n",
    "out=net(inpu)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin UCI digits auto-reduce-viz demo job \n",
      "\n",
      "Creating 64-16-2-16-63 autoencoder \n",
      "\n",
      "bat_size =   3 \n",
      "loss = L1Loss()\n",
      "optimizer = Adam\n",
      "max_epochs = 250 \n",
      "lrn_rate = 0.001 \n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/250]: 100%|██████████| 8/8 [00:19<00:00,  2.41s/it, loss=633503997725.75, mseloss=1168758002.7356377]\n",
      "Epoch [1/250]: 100%|██████████| 8/8 [00:19<00:00,  2.48s/it, loss=29240296.0, mseloss=0.2985072545707226] \n",
      "Epoch [2/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=25584838.0, mseloss=0.24316352047026157] \n",
      "Epoch [3/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=15551318.25, mseloss=0.10193636640906334] \n",
      "Epoch [4/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=7477436.4375, mseloss=0.02845945581793785] \n",
      "Epoch [5/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=4911492.75, mseloss=0.01438736799173057]    \n",
      "Epoch [6/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=4355901.71875, mseloss=0.012429957278072834]\n",
      "Epoch [7/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=4094049.5625, mseloss=0.01120148308109492]   \n",
      "Epoch [8/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=4245861.125, mseloss=0.01195232686586678]   \n",
      "Epoch [9/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=4066567.0, mseloss=0.01152157864999026]    \n",
      "Epoch [10/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=4181018.78125, mseloss=0.01128475007135421] \n",
      "Epoch [11/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3990140.40625, mseloss=0.010914265178143978]\n",
      "Epoch [12/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3690236.96875, mseloss=0.009840128943324089]\n",
      "Epoch [13/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3853822.625, mseloss=0.010260131792165339]   \n",
      "Epoch [14/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3664631.75, mseloss=0.009439748711884022]    \n",
      "Epoch [15/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3657409.4375, mseloss=0.009167586918920279]  \n",
      "Epoch [16/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3697479.421875, mseloss=0.009049900516401976]\n",
      "Epoch [17/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3585542.5625, mseloss=0.008468707033898681]  \n",
      "Epoch [18/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3490763.625, mseloss=0.008163313032127917]  \n",
      "Epoch [19/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3698280.0625, mseloss=0.008410577254835516] \n",
      "Epoch [20/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3654382.5, mseloss=0.008450886234641075]    \n",
      "Epoch [21/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3503741.59375, mseloss=0.007795182464178652]\n",
      "Epoch [22/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3504759.65625, mseloss=0.007587778149172664]\n",
      "Epoch [23/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3706827.0625, mseloss=0.007982767012435943]\n",
      "Epoch [24/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3345657.734375, mseloss=0.00678138475632295]\n",
      "Epoch [25/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3500269.46875, mseloss=0.007413259270833805]\n",
      "Epoch [26/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3421967.0, mseloss=0.0072902614483609796]    \n",
      "Epoch [27/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3413848.40625, mseloss=0.006957452365895733] \n",
      "Epoch [28/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3278181.78125, mseloss=0.006332723773084581] \n",
      "Epoch [29/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3318894.21875, mseloss=0.006747126404661685] \n",
      "Epoch [30/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3255401.453125, mseloss=0.006279199151322246]\n",
      "Epoch [31/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3274625.625, mseloss=0.006227547564776614]  \n",
      "Epoch [32/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3345500.3125, mseloss=0.006343501765513793]\n",
      "Epoch [33/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3156720.09375, mseloss=0.005753634206485003] \n",
      "Epoch [34/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3260946.125, mseloss=0.0065469564869999886]\n",
      "Epoch [35/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3534794.5, mseloss=0.007001416175626218]     \n",
      "Epoch [36/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3431217.9375, mseloss=0.006525225151563063]  \n",
      "Epoch [37/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3303245.65625, mseloss=0.006220366922207177]\n",
      "Epoch [38/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3431920.1875, mseloss=0.0067035114916507155]\n",
      "Epoch [39/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3284471.0625, mseloss=0.005823961342684925]  \n",
      "Epoch [40/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3107953.515625, mseloss=0.005803671956527978] \n",
      "Epoch [41/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3082571.421875, mseloss=0.005576400668360293]\n",
      "Epoch [42/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3177955.65625, mseloss=0.005789510061731562] \n",
      "Epoch [43/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3292905.9375, mseloss=0.0061148423701524734] \n",
      "Epoch [44/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3038545.25, mseloss=0.00575810598093085]     \n",
      "Epoch [45/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3505924.3125, mseloss=0.006705914594931528]  \n",
      "Epoch [46/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3083136.53125, mseloss=0.005665132019203156] \n",
      "Epoch [47/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3445386.71875, mseloss=0.006680014310404658] \n",
      "Epoch [48/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3123181.484375, mseloss=0.005778316815849394]\n",
      "Epoch [49/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3120528.25, mseloss=0.005688869801815599]    \n",
      "Epoch [50/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3313489.703125, mseloss=0.00605266317143105] \n",
      "Epoch [51/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3285184.9375, mseloss=0.0064181040215771645]\n",
      "Epoch [52/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3116431.03125, mseloss=0.005714768689358607] \n",
      "Epoch [53/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3120475.65625, mseloss=0.005898270843317732] \n",
      "Epoch [54/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3087371.59375, mseloss=0.005728765623643994]\n",
      "Epoch [55/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3057303.96875, mseloss=0.005415053747128695] \n",
      "Epoch [56/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3103992.46875, mseloss=0.005748129857238382] \n",
      "Epoch [57/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3067907.8125, mseloss=0.005716649116948247] \n",
      "Epoch [58/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3079951.875, mseloss=0.0055271650780923665]\n",
      "Epoch [59/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3111953.5625, mseloss=0.0055914892291184515] \n",
      "Epoch [60/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3401173.625, mseloss=0.006680281454464421]   \n",
      "Epoch [61/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3186231.6875, mseloss=0.005691754777217284] \n",
      "Epoch [62/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3281454.625, mseloss=0.00597355846548453]    \n",
      "Epoch [63/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3085211.84375, mseloss=0.005517282348591834]\n",
      "Epoch [64/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3152914.34375, mseloss=0.005699262110283598]\n",
      "Epoch [65/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3152720.765625, mseloss=0.005710041616111994]\n",
      "Epoch [66/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3063731.203125, mseloss=0.005415810679551214]\n",
      "Epoch [67/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3060870.703125, mseloss=0.005508252885192633]\n",
      "Epoch [68/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3230177.5, mseloss=0.005978413188131526]    \n",
      "Epoch [69/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3147793.5, mseloss=0.005944692762568593]    \n",
      "Epoch [70/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3121737.703125, mseloss=0.005655319895595312]\n",
      "Epoch [71/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3154886.34375, mseloss=0.005790855240775272] \n",
      "Epoch [72/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3115553.125, mseloss=0.005677613255102187]   \n",
      "Epoch [73/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3237552.75, mseloss=0.006101956154452637]    \n",
      "Epoch [74/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3244936.859375, mseloss=0.005976480053504929]\n",
      "Epoch [75/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3208122.65625, mseloss=0.00595230795443058]  \n",
      "Epoch [76/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3583510.6875, mseloss=0.007108297140803188] \n",
      "Epoch [77/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3339083.5, mseloss=0.006328647170448676]     \n",
      "Epoch [78/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3549665.46875, mseloss=0.006742240278981626]\n",
      "Epoch [79/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3261368.96875, mseloss=0.006009201344568282] \n",
      "Epoch [80/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3398364.9375, mseloss=0.0064102815522346646]\n",
      "Epoch [81/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3357809.5, mseloss=0.0065922085486818105]   \n",
      "Epoch [82/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3268155.875, mseloss=0.006170074135297909]  \n",
      "Epoch [83/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3022581.9375, mseloss=0.005343533324776217] \n",
      "Epoch [84/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3071753.375, mseloss=0.005442483321530744]   \n",
      "Epoch [85/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3113470.5, mseloss=0.005815851240186021]     \n",
      "Epoch [86/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3151814.265625, mseloss=0.00579805881716311]\n",
      "Epoch [87/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3148510.71875, mseloss=0.005777947197202593] \n",
      "Epoch [88/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3088550.8125, mseloss=0.005552240239921957] \n",
      "Epoch [89/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3135401.40625, mseloss=0.0059017066087108105]\n",
      "Epoch [90/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3045016.78125, mseloss=0.005548923450987786]\n",
      "Epoch [91/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3195933.078125, mseloss=0.005741494271205738]\n",
      "Epoch [92/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3211297.46875, mseloss=0.005757465929491445] \n",
      "Epoch [93/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3259248.40625, mseloss=0.006041679211193696]\n",
      "Epoch [94/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3144940.359375, mseloss=0.005707894772058353]\n",
      "Epoch [95/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3271020.75, mseloss=0.006018431216944009]   \n",
      "Epoch [96/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3043288.71875, mseloss=0.005499921826412901] \n",
      "Epoch [97/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3085115.046875, mseloss=0.005491463583894074]\n",
      "Epoch [98/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3179208.875, mseloss=0.005964994779787958]   \n",
      "Epoch [99/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3127371.296875, mseloss=0.005644512071739882]\n",
      "Epoch [100/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3182464.53125, mseloss=0.006022248009685427] \n",
      "Epoch [101/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3136983.390625, mseloss=0.005548059125430882]\n",
      "Epoch [102/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3112174.953125, mseloss=0.005525429412955418]\n",
      "Epoch [103/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3199503.109375, mseloss=0.006096103199524805] \n",
      "Epoch [104/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3314921.4375, mseloss=0.0061098572332412004] \n",
      "Epoch [105/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3208435.84375, mseloss=0.005771108204498887]\n",
      "Epoch [106/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3145242.96875, mseloss=0.005884555692318827] \n",
      "Epoch [107/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3200052.25, mseloss=0.006005782313877717]   \n",
      "Epoch [108/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3130448.875, mseloss=0.005704321782104671]  \n",
      "Epoch [109/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3059434.96875, mseloss=0.005422210757387802]\n",
      "Epoch [110/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3064550.203125, mseloss=0.005412289669038728]\n",
      "Epoch [111/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3105643.875, mseloss=0.005868026579264551]   \n",
      "Epoch [112/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3207806.34375, mseloss=0.00605402301880531]  \n",
      "Epoch [113/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3061105.65625, mseloss=0.005510694201802835]\n",
      "Epoch [114/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3203830.25, mseloss=0.005889277148526162]  \n",
      "Epoch [115/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3081805.0625, mseloss=0.0055347076267935336]\n",
      "Epoch [116/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3075298.03125, mseloss=0.005447514326078817]\n",
      "Epoch [117/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3168109.71875, mseloss=0.005958287947578356]  \n",
      "Epoch [118/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3271261.296875, mseloss=0.006107708089984953]\n",
      "Epoch [119/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3070520.71875, mseloss=0.005685072275809944] \n",
      "Epoch [120/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3025789.09375, mseloss=0.0053022993670310825]\n",
      "Epoch [121/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3044801.625, mseloss=0.005598867923254147]   \n",
      "Epoch [122/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3079602.71875, mseloss=0.0054965532617643476]\n",
      "Epoch [123/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3188384.8125, mseloss=0.005872439738595858]  \n",
      "Epoch [124/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3085028.484375, mseloss=0.005509323556907475]\n",
      "Epoch [125/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3142313.03125, mseloss=0.005749280157033354] \n",
      "Epoch [126/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3197867.25, mseloss=0.005818324658321217]    \n",
      "Epoch [127/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3280380.28125, mseloss=0.006074760662158951]\n",
      "Epoch [128/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3186903.0, mseloss=0.005840684520080686]     \n",
      "Epoch [129/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3101123.28125, mseloss=0.005671892606187612] \n",
      "Epoch [130/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3129564.265625, mseloss=0.0057505723380018026]\n",
      "Epoch [131/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3098997.53125, mseloss=0.005538572382647544] \n",
      "Epoch [132/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3106964.71875, mseloss=0.005775519035523757]\n",
      "Epoch [133/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3159563.234375, mseloss=0.005643093667458743]\n",
      "Epoch [134/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3083719.65625, mseloss=0.005628234095638618]\n",
      "Epoch [135/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3073355.5, mseloss=0.00565475964685902]      \n",
      "Epoch [136/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3092825.59375, mseloss=0.005449646763736382] \n",
      "Epoch [137/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3103863.5625, mseloss=0.005664620141033083]  \n",
      "Epoch [138/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3155719.0625, mseloss=0.00605358459870331]  \n",
      "Epoch [139/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3613488.90625, mseloss=0.0071764521999284625]\n",
      "Epoch [140/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3182839.5, mseloss=0.006112970149843022]     \n",
      "Epoch [141/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3080637.4375, mseloss=0.005556401389185339]   \n",
      "Epoch [142/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3108766.15625, mseloss=0.005596372706349939]\n",
      "Epoch [143/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3133481.78125, mseloss=0.00567804864840582]  \n",
      "Epoch [144/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3113771.0, mseloss=0.005828029243275523]    \n",
      "Epoch [145/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3176349.28125, mseloss=0.005887947510927916] \n",
      "Epoch [146/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3139821.640625, mseloss=0.005582918471191078]\n",
      "Epoch [147/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3279521.875, mseloss=0.005930624785833061]   \n",
      "Epoch [148/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3142442.109375, mseloss=0.00569780997466296] \n",
      "Epoch [149/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3066530.875, mseloss=0.005660225957399234]  \n",
      "Epoch [150/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3070031.609375, mseloss=0.005473488970892504]\n",
      "Epoch [151/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3388519.125, mseloss=0.006384601001627743]   \n",
      "Epoch [152/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3259072.0, mseloss=0.00611012545414269]      \n",
      "Epoch [153/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3256826.0625, mseloss=0.0060844094841741025]\n",
      "Epoch [154/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3142522.53125, mseloss=0.005596195871476084] \n",
      "Epoch [155/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3044494.71875, mseloss=0.0056046624085865915]\n",
      "Epoch [156/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3146186.015625, mseloss=0.00571509514702484]\n",
      "Epoch [157/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3046897.859375, mseloss=0.005422146758064628]\n",
      "Epoch [158/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3092601.78125, mseloss=0.005623087577987462] \n",
      "Epoch [159/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3035608.109375, mseloss=0.005535065691219643]\n",
      "Epoch [160/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3131592.9375, mseloss=0.005874747905181721] \n",
      "Epoch [161/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3164810.4375, mseloss=0.0059412177943158895]\n",
      "Epoch [162/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3093782.65625, mseloss=0.005797863064799458]\n",
      "Epoch [163/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3080460.53125, mseloss=0.005570695124333724] \n",
      "Epoch [164/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3163454.40625, mseloss=0.005748965573729947] \n",
      "Epoch [165/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3132550.8125, mseloss=0.005609062354778871] \n",
      "Epoch [166/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3128499.65625, mseloss=0.005676342581864446]\n",
      "Epoch [167/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3386823.0625, mseloss=0.006447083636885509]  \n",
      "Epoch [168/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3381682.1875, mseloss=0.006608475319808349]  \n",
      "Epoch [169/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3805364.6875, mseloss=0.0077473363489843905] \n",
      "Epoch [170/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3200792.25, mseloss=0.005745516944443807]    \n",
      "Epoch [171/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3512792.65625, mseloss=0.006524030119180679] \n",
      "Epoch [172/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3507011.953125, mseloss=0.006781964446417987]\n",
      "Epoch [173/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3338142.6875, mseloss=0.006485142279416323]  \n",
      "Epoch [174/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3062814.625, mseloss=0.005534526193514466]   \n",
      "Epoch [175/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3159300.6875, mseloss=0.005907089653192088] \n",
      "Epoch [176/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3250551.75, mseloss=0.00609791450551711]     \n",
      "Epoch [177/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3151522.890625, mseloss=0.0057209797378163785]\n",
      "Epoch [178/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3197431.59375, mseloss=0.005984497722238302]\n",
      "Epoch [179/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3134292.3125, mseloss=0.005723782116547227]  \n",
      "Epoch [180/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3102597.25, mseloss=0.005811760755022988]    \n",
      "Epoch [181/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3100221.265625, mseloss=0.00550568348262459]\n",
      "Epoch [182/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3123624.03125, mseloss=0.005693223152775317]\n",
      "Epoch [183/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3114102.125, mseloss=0.00558902183547616]   \n",
      "Epoch [184/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3135533.40625, mseloss=0.005795208504423499]\n",
      "Epoch [185/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3066274.75, mseloss=0.005400441033998504]    \n",
      "Epoch [186/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3088903.40625, mseloss=0.005533274437766522] \n",
      "Epoch [187/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3055099.015625, mseloss=0.005418015236500651] \n",
      "Epoch [188/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3048103.46875, mseloss=0.00538205448538065]  \n",
      "Epoch [189/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3094642.21875, mseloss=0.005645026249112561] \n",
      "Epoch [190/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3166469.390625, mseloss=0.005754133337177336]\n",
      "Epoch [191/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3061096.28125, mseloss=0.00563628610689193] \n",
      "Epoch [192/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3070938.1875, mseloss=0.005645914206979796] \n",
      "Epoch [193/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3213071.84375, mseloss=0.0057933119824156165]\n",
      "Epoch [194/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3133004.03125, mseloss=0.006005702598486096]\n",
      "Epoch [195/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3326067.71875, mseloss=0.006442412995966151] \n",
      "Epoch [196/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3000248.75, mseloss=0.005318374227499589]   \n",
      "Epoch [197/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3067833.625, mseloss=0.005766311980551109]   \n",
      "Epoch [198/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3326021.34375, mseloss=0.006308818061370403] \n",
      "Epoch [199/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3092489.9375, mseloss=0.00572812685277313]   \n",
      "Epoch [200/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3088916.4375, mseloss=0.0054220503370743245]\n",
      "Epoch [201/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3038195.828125, mseloss=0.005294530332321301]\n",
      "Epoch [202/250]: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it, loss=3132412.125, mseloss=0.00568428062251769]    \n",
      "Epoch [203/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3103801.28125, mseloss=0.005548663990339264]\n",
      "Epoch [204/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3381440.375, mseloss=0.006348687864374369]   \n",
      "Epoch [205/250]: 100%|██████████| 8/8 [00:19<00:00,  2.47s/it, loss=3306099.125, mseloss=0.006222363299457356]   \n",
      "Epoch [206/250]:  25%|██▌       | 2/8 [00:06<00:18,  3.15s/it, loss=596845.90625, mseloss=0.0007694211672060192]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000005vscode-remote?line=38'>39</a>\u001b[0m mseloss\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000005vscode-remote?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m (batch_idx, batch) \u001b[39min\u001b[39;00m loop:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000005vscode-remote?line=40'>41</a>\u001b[0m   X \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39;49mto(device)  \u001b[39m# no targets needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000005vscode-remote?line=42'>43</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000005vscode-remote?line=43'>44</a>\u001b[0m   oupt \u001b[39m=\u001b[39m net(X)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "\n",
    "force_cudnn_initialization()\n",
    "# 0. setup\n",
    "print(\"\\nBegin UCI digits auto-reduce-viz demo job \")\n",
    "T.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "bat_size = 3\n",
    "train_ldr = T.utils.data.DataLoader(crop_images,\n",
    "batch_size=bat_size, shuffle=True)\n",
    "train_ldr\n",
    "  # 2. create network\n",
    "print(\"\\nCreating 64-16-2-16-63 autoencoder \")\n",
    "net = Net().to(\"cuda:0\")\n",
    "\n",
    "# 3. train model\n",
    "max_epochs = 250\n",
    "ep_log_interval = 10\n",
    "lrn_rate = 0.001\n",
    "\n",
    "loss_func1 = T.nn.MSELoss()\n",
    "loss_func = T.nn.L1Loss(reduction='sum')\n",
    "optimizer = T.optim.Adam(net.parameters(), lr=lrn_rate)\n",
    "\n",
    "print(\"\\nbat_size = %3d \" % bat_size)\n",
    "print(\"loss = \" + str(loss_func))\n",
    "print(\"optimizer = Adam\")\n",
    "print(\"max_epochs = %3d \" % max_epochs)\n",
    "print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "\n",
    "print(\"\\nStarting training\")\n",
    "net = net.train()\n",
    "\n",
    "\n",
    "for epoch in range(0, max_epochs):\n",
    "  loop= tqdm(enumerate(train_ldr), total=len(train_ldr),leave=True)\n",
    "  epoch_loss = 0  # for one full epoch\n",
    "  mseloss=0\n",
    "  for (batch_idx, batch) in loop:\n",
    "    X = batch.to(device)  # no targets needed\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    oupt = net(X)\n",
    "    loss_obj = loss_func(oupt, X)  # note: X not Y\n",
    "    loss_obj1=loss_func1(oupt,X)\n",
    "    epoch_loss += loss_obj.item()  # accumulate\n",
    "    mseloss+=loss_obj1.item()\n",
    "    loss_obj.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loop.set_description(f\"Epoch [{epoch}/{max_epochs}]\")\n",
    "    loop.set_postfix(loss=str(epoch_loss),mseloss=str(mseloss))\n",
    "\"\"\"\n",
    "  if epoch % ep_log_interval == 0:\n",
    "    print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss)) \n",
    "print(\"Done \")\"\"\"\n",
    "\n",
    "# 4. plot digits using reduced form\n",
    "print(\"\\nCreating graph from encoded data \")\n",
    "net = net.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), 'hari_model_sample.pth')\n",
    "torch.save(net, 'hari_model_struct.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oupt.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('hari_model_struct.pth')\n",
    "\"\"\"net=Net()\n",
    "net.load_state_dict(torch.load('hari_model_sample.pth'))\n",
    "net.to(device)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 8.00 GiB total capacity; 6.09 GiB already allocated; 0 bytes free; 7.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m train_ldr:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000009vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m out\u001b[39m=\u001b[39mnet(i\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000009vscode-remote?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(i[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/han/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/han/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/han/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/han/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/han/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/han/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/han/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb Cell 4'\u001b[0m in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=62'>63</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=64'>65</a>\u001b[0m   z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=65'>66</a>\u001b[0m   z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=66'>67</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[1;32m/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb Cell 4'\u001b[0m in \u001b[0;36mNet.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=41'>42</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc5(z))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=42'>43</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc6(z))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=43'>44</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc7(z))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=44'>45</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc8(z))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000003vscode-remote?line=45'>46</a>\u001b[0m z \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc9(z))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 8.00 GiB total capacity; 6.09 GiB already allocated; 0 bytes free; 7.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for i in train_ldr:\n",
    "    break\n",
    "out=net(i.to(device))\n",
    "plt.imshow(i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa955cfa580>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out[1][0].to(\"cpu\").detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Work/Autoenc/hyperspectral-autoencoders-master/hyperspectral-autoencoders-master/autoenc_scratch/dimensenc-hyp-v2.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m loss_func(out[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m), i[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "loss_func(out[0][0].to(\"cpu\"), i[0][0].to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=crop_images.to(device)\n",
    "xy = net.encode(bow).to(device)\n",
    "xy = xy.to(\"cpu\").detach().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  with T.no_grad():\n",
    "    bow=train_ds[:]['pixels']\n",
    "    bow=crop_images.to(device)\n",
    "    xy = net.encode(bow)  # (1797,2)\n",
    "  lbls = train_ds[:]['digit']  # (1797,)\n",
    "  lbls=lbls.to(device)\n",
    "  xy = xy.to(\"cpu\").detach().numpy()      # tensors to numpy arrays\n",
    "  lbls = lbls.to(\"cpu\").detach().numpy()\n",
    "  \n",
    "  \n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  colors = ['red', 'blue', 'green', 'yellow', 'orange',\n",
    "            'black', 'brown', 'purple', 'silver', 'pink']\n",
    "  digits = [' 0 ', ' 1 ', ' 2 ', ' 3 ', ' 4 ',\n",
    "            ' 5 ', ' 6 ', ' 7 ', ' 8 ', ' 9 ']\n",
    "\n",
    "  # process by each color/class\n",
    "  for i in range(len(colors)):  # 0 to 9 each color\n",
    "    color = colors[i]\n",
    "    rows = []                   # select rows for curr color\n",
    "    for j in range(len(xy)):    # 0 to 1796\n",
    "      if lbls[j] == i:\n",
    "        rows.append(True)       # to extract rows, must use bool\n",
    "      else:\n",
    "        rows.append(False)\n",
    "\n",
    "    rows = np.array(rows, dtype=np.bool)  # list to array\n",
    "    selected = xy[rows,:]                 # like (178,2)\n",
    "\n",
    "    x = selected[:,0]  # like (178,)\n",
    "    y = selected[:,1]\n",
    "    scatter  = ax.scatter(x, y, c=color, s=20, alpha=0.9)\n",
    "    # and continue on to next color/class\n",
    "\n",
    "  txt = \"\\n\" + \"0=red \\n\" + \"1=blue \\n\" + \"2=green \\n\" + \\\n",
    "    \"3=yellow \\n\" + \"4=orange \\n\" + \"5=black \\n\" + \\\n",
    "    \"6=brown \\n\" + \"7=purple \\n\" + \"8=silver \\n\" + \"9=pink \\n\"\n",
    "\n",
    "  props = dict(boxstyle='round', facecolor='wheat', alpha=0.95)\n",
    "  ax.text(0.95, 0.95, txt, transform=ax.transAxes, fontsize=8,\n",
    "    verticalalignment='top', bbox=props)\n",
    "  ax.grid(True)\n",
    "  plt.xlabel('component 1')\n",
    "  plt.ylabel('component 2')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"\\nEnd UCI digits auto-reduce-viz demo\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"digits_uci_test_1797.txt\"\n",
    "train_ds = UciDigitsDataset(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "# 0. setup\n",
    "print(\"\\nBegin UCI digits auto-reduce-viz demo job \")\n",
    "T.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# 1. create DataLoader object\n",
    "print(\"\\nCreating UCI Digits Dataset \")\n",
    "\n",
    "train_file = \"digits_uci_test_1797.txt\"\n",
    "train_ds = UciDigitsDataset(train_file) # all 1797 rows\n",
    "\n",
    "\n",
    "bat_size = 10\n",
    "train_ldr = T.utils.data.DataLoader(train_ds,\n",
    "batch_size=bat_size, shuffle=True)\n",
    "\n",
    "# 2. create network\n",
    "print(\"\\nCreating 64-16-2-16-63 autoencoder \")\n",
    "net = Net().to(\"cuda:0\")\n",
    "\n",
    "# 3. train model\n",
    "max_epochs = 200\n",
    "ep_log_interval = 10\n",
    "lrn_rate = 0.01\n",
    "\n",
    "loss_func = T.nn.MSELoss()\n",
    "optimizer = T.optim.Adam(net.parameters(), lr=lrn_rate)\n",
    "\n",
    "print(\"\\nbat_size = %3d \" % bat_size)\n",
    "print(\"loss = \" + str(loss_func))\n",
    "print(\"optimizer = Adam\")\n",
    "print(\"max_epochs = %3d \" % max_epochs)\n",
    "print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "\n",
    "print(\"\\nStarting training\")\n",
    "net = net.train()\n",
    "for epoch in range(0, max_epochs):\n",
    "    epoch_loss = 0  # for one full epoch\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(train_ldr):\n",
    "        X = batch['pixels'].to(device)  # no targets needed\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        oupt = net(X)\n",
    "        loss_obj = loss_func(oupt, X)  # note: X not Y\n",
    "        epoch_loss += loss_obj.item()  # accumulate\n",
    "        loss_obj.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % ep_log_interval == 0:\n",
    "        print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))\n",
    "print(\"Done \")\n",
    "\n",
    "# 4. plot digits using reduced form\n",
    "print(\"\\nCreating graph from encoded data \")\n",
    "net = net.eval()\n",
    "\n",
    "with T.no_grad():\n",
    "    bow=train_ds[:]['pixels']\n",
    "    bow=bow.to(device)\n",
    "    xy = net.encode(bow)  # (1797,2)\n",
    "    lbls = train_ds[:]['digit']  # (1797,)\n",
    "    lbls=lbls.to(device)\n",
    "    xy = xy.to(\"cpu\").detach().numpy()      # tensors to numpy arrays\n",
    "    lbls = lbls.to(\"cpu\").detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "colors = ['red', 'blue', 'green', 'yellow', 'orange',\n",
    "        'black', 'brown', 'purple', 'silver', 'pink']\n",
    "digits = [' 0 ', ' 1 ', ' 2 ', ' 3 ', ' 4 ',\n",
    "        ' 5 ', ' 6 ', ' 7 ', ' 8 ', ' 9 ']\n",
    "\n",
    "# process by each color/class\n",
    "for i in range(len(colors)):  # 0 to 9 each color\n",
    "    color = colors[i]\n",
    "rows = []                   # select rows for curr color\n",
    "for j in range(len(xy)):    # 0 to 1796\n",
    "    if lbls[j] == i:\n",
    "        rows.append(True)       # to extract rows, must use bool\n",
    "    else:\n",
    "        rows.append(False)\n",
    "\n",
    "rows = np.array(rows, dtype=np.bool)  # list to array\n",
    "selected = xy[rows,:]                 # like (178,2)\n",
    "\n",
    "x = selected[:,0]  # like (178,)\n",
    "y = selected[:,1]\n",
    "scatter  = ax.scatter(x, y, c=color, s=20, alpha=0.9)\n",
    "# and continue on to next color/class\n",
    "\n",
    "txt = \"\\n\" + \"0=red \\n\" + \"1=blue \\n\" + \"2=green \\n\" + \\\n",
    "\"3=yellow \\n\" + \"4=orange \\n\" + \"5=black \\n\" + \\\n",
    "\"6=brown \\n\" + \"7=purple \\n\" + \"8=silver \\n\" + \"9=pink \\n\"\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.95)\n",
    "ax.text(0.95, 0.95, txt, transform=ax.transAxes, fontsize=8,\n",
    "verticalalignment='top', bbox=props)\n",
    "ax.grid(True)\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEnd UCI digits auto-reduce-viz demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xy[:,0],xy[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
